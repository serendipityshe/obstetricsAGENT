from starlette.responses import JSONResponse
from fastapi import APIRouter, HTTPException, status, Depends, Form, Path, Query, UploadFile, File, Body
from pydantic import BaseModel, Field
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from typing import Any, Optional, List, Union, Literal, AsyncGenerator
from datetime import date, datetime, timedelta

import json
import uuid
import os
import mimetypes
import time
import asyncio
from asyncio import Semaphore, Queue
from backend.workflow.test import prengant_workflow, PrengantState
from backend.api.v1.services.maternal_service import MaternalService  # å¤ç”¨æœåŠ¡å±‚
# å¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨ç›¸å…³å¯¼å…¥å·²ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨æµå¼å¤„ç†
import logging
from collections import defaultdict, deque
from dataclasses import dataclass
from threading import Lock

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger("pregnant-workflow-api")

# ==============================
# P0çº§æ€§èƒ½ä¼˜åŒ–é…ç½®
# ==============================

# 1. å¹¶å‘æ§åˆ¶é…ç½®
MAX_CONCURRENT_REQUESTS = 20  # æœ€å¤§å¹¶å‘å¤„ç†æ•°é‡
WORKFLOW_TIMEOUT = 90.0      # å·¥ä½œæµè¶…æ—¶æ—¶é—´(ç§’)
QUEUE_MAX_SIZE = 50          # è¯·æ±‚é˜Ÿåˆ—æœ€å¤§å¤§å°

# 2. å…¨å±€å¹¶å‘æ§åˆ¶ä¿¡å·é‡
workflow_semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)

# 3. è¯·æ±‚é˜Ÿåˆ—
request_queue: Queue = Queue(maxsize=QUEUE_MAX_SIZE)

# 4. è¯·æ±‚é˜Ÿåˆ—å¤„ç†å™¨
async def process_queue_requests():
    """å¤„ç†è¯·æ±‚é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡"""
    while True:
        try:
            # ç­‰å¾…é˜Ÿåˆ—ä¸­çš„è¯·æ±‚
            request_item = await request_queue.get()
            request_data, response_future = request_item

            # æ‰§è¡Œè¯·æ±‚
            try:
                result = await execute_workflow_stream_protected(request_data)
                response_future.set_result(result)
            except Exception as e:
                response_future.set_exception(e)
            finally:
                request_queue.task_done()

        except Exception as e:
            logger.error(f"é˜Ÿåˆ—å¤„ç†å™¨å¼‚å¸¸: {e}")
            await asyncio.sleep(1)  # é˜²æ­¢æ— é™å¾ªç¯

# å¯åŠ¨é˜Ÿåˆ—å¤„ç†å™¨
async def start_queue_processor():
    """å¯åŠ¨è¯·æ±‚é˜Ÿåˆ—å¤„ç†å™¨"""
    asyncio.create_task(process_queue_requests())

async def wait_for_queue_result(response_future: asyncio.Future):
    """ç­‰å¾…é˜Ÿåˆ—å¤„ç†ç»“æœå¹¶æµå¼è¿”å›"""
    try:
        # ç­‰å¾…é˜Ÿåˆ—å¤„ç†å®Œæˆ
        result_generator = await response_future

        # æµå¼è¿”å›ç»“æœ
        async for chunk in result_generator:
            yield chunk

    except Exception as e:
        logger.error(f"é˜Ÿåˆ—ç»“æœç­‰å¾…å¼‚å¸¸ï¼š{e}")
        yield f"{json.dumps({'type': 'error', 'message': f'é˜Ÿåˆ—å¤„ç†å¤±è´¥: {str(e)}'}, ensure_ascii=False)}\n"
        yield f"{json.dumps({'type': 'done'}, ensure_ascii=False)}\n"

# 5. æ€§èƒ½ç›‘æ§æ•°æ®ç»“æ„
@dataclass
class PerformanceMetrics:
    active_requests: int = 0
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    timeout_requests: int = 0
    queue_waiting: int = 0
    avg_response_time: float = 0.0
    recent_response_times: deque = None

    def __post_init__(self):
        if self.recent_response_times is None:
            self.recent_response_times = deque(maxlen=100)

# 6. å…¨å±€æ€§èƒ½ç›‘æ§å®ä¾‹
performance_metrics = PerformanceMetrics()
metrics_lock = Lock()

def update_metrics(success: bool, duration: float, timeout: bool = False):
    """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
    with metrics_lock:
        performance_metrics.total_requests += 1
        if success:
            performance_metrics.successful_requests += 1
        else:
            performance_metrics.failed_requests += 1

        if timeout:
            performance_metrics.timeout_requests += 1

        performance_metrics.recent_response_times.append(duration)

        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        if performance_metrics.recent_response_times:
            performance_metrics.avg_response_time = sum(performance_metrics.recent_response_times) / len(performance_metrics.recent_response_times)

def increment_active_requests():
    """å¢åŠ æ´»è·ƒè¯·æ±‚è®¡æ•°"""
    with metrics_lock:
        performance_metrics.active_requests += 1

def decrement_active_requests():
    """å‡å°‘æ´»è·ƒè¯·æ±‚è®¡æ•°"""
    with metrics_lock:
        performance_metrics.active_requests -= 1

def update_queue_waiting(count: int):
    """æ›´æ–°é˜Ÿåˆ—ç­‰å¾…æ•°é‡"""
    with metrics_lock:
        performance_metrics.queue_waiting = count

def get_performance_snapshot():
    """è·å–æ€§èƒ½æŒ‡æ ‡å¿«ç…§"""
    with metrics_lock:
        return {
            "active_requests": performance_metrics.active_requests,
            "total_requests": performance_metrics.total_requests,
            "successful_requests": performance_metrics.successful_requests,
            "failed_requests": performance_metrics.failed_requests,
            "timeout_requests": performance_metrics.timeout_requests,
            "success_rate": f"{(performance_metrics.successful_requests / max(performance_metrics.total_requests, 1)) * 100:.1f}%",
            "avg_response_time": f"{performance_metrics.avg_response_time:.2f}s",
            "queue_waiting": performance_metrics.queue_waiting,
            "queue_size": request_queue.qsize(),
            "semaphore_available": workflow_semaphore._value
        }

# åˆå§‹åŒ–è·¯ç”±ï¼ˆæ ‡ç­¾ä¸ç°æœ‰èŠå¤©ç®¡ç†æœåŠ¡åˆ†ç±»ä¸€è‡´ï¼‰
router = APIRouter(tags=["èŠå¤©ç®¡ç†æœåŠ¡"])
maternal_service = MaternalService()

# åˆå§‹åŒ–æ ‡å¿—ï¼Œç¡®ä¿é˜Ÿåˆ—å¤„ç†å™¨åªå¯åŠ¨ä¸€æ¬¡
_queue_processor_started = False

async def ensure_queue_processor_started():
    """ç¡®ä¿è¯·æ±‚é˜Ÿåˆ—å¤„ç†å™¨å·²å¯åŠ¨"""
    global _queue_processor_started
    if not _queue_processor_started:
        await start_queue_processor()
        _queue_processor_started = True
        logger.info("è¯·æ±‚é˜Ÿåˆ—å¤„ç†å™¨å·²å¯åŠ¨")

# ------------------------------
# 1. é€šç”¨å·¥å…·å‡½æ•°ï¼ˆè¾…åŠ©ç”ŸæˆURLã€æ–‡ä»¶ä¿¡æ¯ç­‰ï¼‰
# ------------------------------
# æ³¨æ„ï¼šç”±äºè¿”å›æ ¼å¼ç®€åŒ–ä¸ºfile_idï¼Œä»¥ä¸‹å‡½æ•°å·²ä¸å†ä½¿ç”¨ï¼Œä¿ç•™ä½œä¸ºå‚è€ƒ
# def generate_temp_token() -> str:
#     """ç”Ÿæˆä¸´æ—¶URLä»¤ç‰Œï¼ˆå®é™…é¡¹ç›®éœ€å¯¹æ¥æ–‡ä»¶æœåŠ¡ç”Ÿæˆæœ‰æ•ˆä»¤ç‰Œï¼‰"""
#     return str(uuid.uuid4())[:8]  # å–UUIDå‰8ä½ä½œä¸ºä¸´æ—¶ä»¤ç‰Œ
# 
# def get_file_size(file_path: str) -> int:
#     """è·å–æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œè·¯å¾„ä¸å­˜åœ¨æ—¶è¿”å›0"""
#     try:
#         return os.path.getsize(file_path) if file_path and os.path.exists(file_path) else 0
#     except Exception as e:
#         logger.warning(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {e}")
#         return 0
# 
# def generate_expire_time(days: int = 7) -> str:
#     """ç”Ÿæˆæ–‡ä»¶è¿‡æœŸæ—¶é—´ï¼ˆå½“å‰æ—¶é—´+Nå¤©ï¼‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS"""
#     expire_dt = datetime.now() + timedelta(days=days)
#     return expire_dt.strftime("%Y-%m-%dT%H:%M:%S.%fZ")
# 
# def get_file_name_from_path(file_path: str) -> str:
#     """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ–‡ä»¶åï¼ˆå¦‚æ— è·¯å¾„åˆ™è¿”å›é»˜è®¤åï¼‰"""
#     if not file_path:
#         return "unknown_file"
#     return os.path.basename(file_path) or "unknown_file"
# 
# def get_file_type_from_path(file_path: str) -> str:
#     """ä»æ–‡ä»¶è·¯å¾„æ¨æ–­MIMEç±»å‹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…é¡¹ç›®éœ€ç”¨ä¸“ä¸šåº“å¦‚python-magicï¼‰"""
#     if not file_path:
#         return "application/octet-stream"
#     ext = os.path.splitext(file_path)[-1].lower()
#     if ext in [".jpg", ".jpeg", ".png", ".gif"]:
#         return f"image/{ext[1:]}"
#     elif ext == ".pdf":
#         return "application/pdf"
#     elif ext in [".doc", ".docx"]:
#         return "application/msword" if ext == ".doc" else "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
#     else:
#         return "application/octet-stream"

# ------------------------------
# 2. åˆ›å»ºchat_idçš„æ¥å£
# ------------------------------
class CreateChatIdJsonRequest(BaseModel):
    """åˆ›å»ºå¯¹è¯IDçš„JSONè¯·æ±‚æ¨¡å‹"""
    maternal_id: int = Field(..., description="å­•å¦‡å”¯ä¸€æ ‡è¯†IDï¼ˆå¿…å¡«ï¼Œç”¨äºå…³è”ä¸ªäººæ•°æ®ï¼‰")
    user_type: str = Field(..., description="ç”¨æˆ·ç±»å‹ï¼ˆå¿…å¡«ï¼Œå›ºå®šå€¼ï¼špregnant_mother/doctorï¼‰")
    
    # å¯é€‰ï¼šæ·»åŠ éªŒè¯ç¡®ä¿user_typeåªèƒ½æ˜¯æŒ‡å®šå€¼
    @classmethod
    def validate_user_type(cls, v):
        if v not in ["pregnant_mother", "doctor"]:
            raise ValueError("user_typeå¿…é¡»æ˜¯'pregnant_mother'æˆ–'doctor'")
        return v

class CreateChatIdRequest(BaseModel):
    """åˆ›å»ºå¯¹è¯IDçš„å“åº”æ¨¡å‹"""
    code: int = Field(..., description="çŠ¶æ€ç ï¼š200=æˆåŠŸï¼Œ500=ç³»ç»Ÿé”™è¯¯")
    msg: str = Field(..., description="æç¤ºä¿¡æ¯")
    data: Optional[dict] = Field(None, description="ä¸šåŠ¡æ•°æ®ï¼ŒåŒ…å«ç”Ÿæˆçš„chat_id")

@router.post(
    "/chat_id",
    description="åˆ›å»ºå¯¹è¯id",
    response_model=CreateChatIdRequest,
    status_code=status.HTTP_200_OK
)
async def create_chat_id(
    request: CreateChatIdJsonRequest
):
    try:
        chat_uuid = str(uuid.uuid4())
        chat_id = f'chat_{request.maternal_id}_{request.user_type}_{chat_uuid}'
        save_chat_path = os.path.join('dataset', 'pregnant_mother', str(request.maternal_id), 'chat')
        os.makedirs(save_chat_path, exist_ok=True)
        json_file_path = os.path.join(save_chat_path, f'{chat_id}.json')
        
        # ç”Ÿæˆå‘é‡å­˜å‚¨è·¯å¾„
        vector_store_path = f"/root/project2/data/vector_store/chat_{chat_id}_maternal_{request.maternal_id}"
        
        try:
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump({}, f)  # å†™å…¥ç©º JSON å¯¹è±¡ï¼Œä¿è¯æ–‡ä»¶æ ¼å¼æ­£ç¡®
            print(f"ç©º JSON æ–‡ä»¶å·²åˆ›å»º: {json_file_path}")
            
            # åˆ›å»ºèŠå¤©è®°å½•å¹¶åŒæ—¶è®¾ç½®å‘é‡å­˜å‚¨è·¯å¾„
            result = maternal_service.dataset_service.create_dialogue(
                maternal_id=request.maternal_id,
                chat_id=chat_id,
                dialogue_content=json_file_path,
                vector_store_path=vector_store_path
            )
            logger.info(f"å·²ä¸º chat_id={chat_id} åˆå§‹åŒ–å‘é‡å­˜å‚¨è·¯å¾„: {vector_store_path}")
            
        except Exception as e:
            print(f"åˆ›å»ºç©º JSON æ–‡ä»¶æˆ–æ•°æ®åº“è®°å½•å¤±è´¥: {str(e)}")
        return CreateChatIdRequest(
            code=200,
            msg="å¯¹è¯IDåˆ›å»ºæˆåŠŸ",
            data={"chat_id": chat_id, "vector_store_path": vector_store_path}
        )
    except Exception as e:
        logger.error(f"åˆ›å»ºå¯¹è¯IDå¤±è´¥: {e}")
        raise HTTPException(
            status_code=500,
            detail=CreateChatIdRequest(
                code=500,
                msg="åˆ›å»ºå¯¹è¯IDå¤±è´¥",
                data=None
            ).model_dump()
        )


# ------------------------------
# 3. å®šä¹‰è¯·æ±‚/å“åº”æ¨¡å‹ï¼ˆPydanticéªŒè¯ï¼‰
# ------------------------------
# 3.1 æ¶ˆæ¯å†…å®¹å­æ¨¡å‹ï¼ˆæ”¯æŒtext/image_url/documentä¸‰ç§ç±»å‹ï¼‰
class TextContent(BaseModel):
    """æ–‡æœ¬ç±»å‹æ¶ˆæ¯å†…å®¹"""
    type: Literal["text"] = Field("text", description="å†…å®¹ç±»å‹ï¼šå›ºå®šä¸ºtext")
    text: str = Field(..., description="æ–‡æœ¬å†…å®¹")

class ImageUrlInfo(BaseModel):
    """å›¾ç‰‡URLä¿¡æ¯"""
    file_id: str = Field(..., description="æ–‡ä»¶ID")

class ImageUrlContent(BaseModel):
    """å›¾ç‰‡ç±»å‹æ¶ˆæ¯å†…å®¹"""
    type: Literal["image_url"] = Field("image_url", description="å†…å®¹ç±»å‹ï¼šå›ºå®šä¸ºimage_url")
    image_url: ImageUrlInfo = Field(..., description="å›¾ç‰‡è¯¦ç»†ä¿¡æ¯")

class DocumentInfo(BaseModel):
    """æ–‡æ¡£URLä¿¡æ¯"""
    file_id: str = Field(..., description="æ–‡ä»¶ID")

class DocumentContent(BaseModel):
    """æ–‡æ¡£ç±»å‹æ¶ˆæ¯å†…å®¹"""
    type: Literal["document"] = Field("document", description="å†…å®¹ç±»å‹ï¼šå›ºå®šä¸ºdocument")
    document: DocumentInfo = Field(..., description="æ–‡æ¡£è¯¦ç»†ä¿¡æ¯")

# æ¶ˆæ¯å†…å®¹ç±»å‹é›†åˆï¼ˆUnionï¼‰
MessageContent = Union[TextContent, ImageUrlContent, DocumentContent]

# 3.2 å•æ¡æ¶ˆæ¯æ¨¡å‹
class MessageItem(BaseModel):
    """å•æ¡å¯¹è¯æ¶ˆæ¯æ¨¡å‹ï¼ˆç”¨æˆ·/åŠ©æ‰‹ï¼‰"""
    message_id: str = Field(..., description="æ¶ˆæ¯å”¯ä¸€IDï¼ˆæ ¼å¼ï¼šmsg_+UUIDï¼‰")
    role: str = Field(..., description="è§’è‰²ï¼šuserï¼ˆç”¨æˆ·ï¼‰/assistantï¼ˆåŠ©æ‰‹ï¼‰")
    content: List[MessageContent] = Field(..., description="æ¶ˆæ¯å†…å®¹åˆ—è¡¨ï¼ˆæ”¯æŒå¤šç±»å‹æ··åˆï¼‰")
    timestamp: str = Field(..., description="æ¶ˆæ¯æ—¶é—´æˆ³ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼‰")

# 3.3 å¯¹è¯å…ƒæ•°æ®æ¨¡å‹
class ChatMeta(BaseModel):
    """å¯¹è¯å…ƒæ•°æ®ï¼ˆæ ‡è¯†å¯¹è¯å½’å±ï¼‰"""
    chat_id: str = Field(..., description="å¯¹è¯IDï¼ˆä¸åˆ›å»ºæ¥å£è¿”å›ä¸€è‡´ï¼‰")
    user_type: str = Field(..., description="ç”¨æˆ·ç±»å‹ï¼ˆpregnant_mother/doctorï¼‰")
    maternal_id: int = Field(..., description="å­•å¦‡å”¯ä¸€æ ‡è¯†ID")

# 3.4 æ¥å£å“åº”æ•°æ®æ¨¡å‹ï¼ˆdataå­—æ®µå†…éƒ¨ç»“æ„ï¼‰
class WorkflowData(BaseModel):
    """å¯¹è¯æ¥å£å“åº”çš„ä¸šåŠ¡æ•°æ®æ¨¡å‹"""
    chat_meta: ChatMeta = Field(..., description="å¯¹è¯å…ƒæ•°æ®")
    session_title: str = Field(..., description="ä¼šè¯æ ‡é¢˜ï¼ˆå–è‡ªç”¨æˆ·é¦–æ¬¡è¾“å…¥ï¼‰")
    messages: List[MessageItem] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼ˆç”¨æˆ·+åŠ©æ‰‹ï¼‰")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯ï¼ˆæ— é”™è¯¯æ—¶ä¸ºnullï¼‰")

# 3.5 æ¥å£è¯·æ±‚æ¨¡å‹ï¼ˆä¿ç•™åŸå­—æ®µï¼Œä¼˜åŒ–æè¿°ï¼‰
class PregnantWorkflowRequest(BaseModel):
    """å­•å¦‡å·¥ä½œæµè°ƒç”¨è¯·æ±‚æ¨¡å‹"""
    input: str = Field(..., description="ç”¨æˆ·è¾“å…¥çš„é—®é¢˜/éœ€æ±‚ï¼ˆå¦‚ï¼šå­•å¦‡æœ€è¿‘å‡ºç°å¤´æ™•ç—‡çŠ¶ï¼Œéœ€è¦ä»€ä¹ˆå»ºè®®ï¼Ÿï¼‰")
    maternal_id: int = Field(..., description="å­•å¦‡å”¯ä¸€æ ‡è¯†IDï¼ˆå¿…å¡«ï¼Œç”¨äºå…³è”ä¸ªäººæ•°æ®ï¼‰")
    chat_id: str = Field(..., description="èŠå¤©ä¼šè¯IDï¼ˆå¿…å¡«ï¼Œä¸åˆ›å»ºæ¥å£è¿”å›ä¸€è‡´ï¼‰")
    user_type: str = Field(..., description="ç”¨æˆ·ç±»å‹ï¼ˆå¿…å¡«ï¼Œå›ºå®šå€¼ï¼špregnant_mother/doctorï¼‰")
    timestamp: str = Field(
        default_factory=lambda: datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
        description="è¯·æ±‚æ—¶é—´æˆ³ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰"
    )
    file_id: Optional[List[str]] = Field(None, description="å…³è”æ–‡ä»¶IDåˆ—è¡¨ï¼ˆæ”¯æŒå›¾ç‰‡/æ–‡æ¡£ï¼‰")

# 3.6 æ¥å£æœ€ç»ˆå“åº”æ¨¡å‹ï¼ˆå®Œå…¨åŒ¹é…ç›®æ ‡æ ¼å¼ï¼‰
class PregnantWorkflowResponse(BaseModel):
    """å­•å¦‡å·¥ä½œæµè°ƒç”¨æœ€ç»ˆå“åº”æ¨¡å‹"""
    code: int = Field(..., description="çŠ¶æ€ç ï¼š200=æˆåŠŸï¼Œ500=å¤±è´¥")
    msg: str = Field(..., description="çŠ¶æ€æè¿°ï¼šsuccess/å¤±è´¥åŸå› ")
    data: WorkflowData = Field(..., description="ä¸šåŠ¡æ•°æ®ï¼ˆå«å¯¹è¯å†…å®¹ï¼‰")

# å¼‚æ­¥ä»»åŠ¡ç›¸å…³æ¨¡å‹å·²ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨æµå¼å“åº”

# ------------------------------
# 4. å·¥ä½œæµæ‰§è¡Œå‡½æ•°ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
# ------------------------------

# åŒæ­¥å·¥ä½œæµå‡½æ•°å·²ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨æµå¼å¤„ç†æ¨¡å¼

# ------------------------------
# æµå¼å·¥ä½œæµæ‰§è¡Œå‡½æ•°
# ------------------------------

async def execute_workflow_stream_protected(request_data: dict) -> AsyncGenerator[str, None]:
    """å¸¦å¹¶å‘æ§åˆ¶å’Œè¶…æ—¶ä¿æŠ¤çš„æµå¼å·¥ä½œæµæ‰§è¡Œ"""
    request_start_time = time.time()
    request_id = f"req_{uuid.uuid4().hex[:8]}"

    # å¢åŠ æ´»è·ƒè¯·æ±‚è®¡æ•°
    increment_active_requests()

    # è®°å½•è¯·æ±‚å¼€å§‹
    logger.info(f"[{request_id}] å¼€å§‹å¤„ç†è¯·æ±‚ - å½“å‰æ´»è·ƒè¯·æ±‚: {performance_metrics.active_requests}")

    try:
        # 1. è·å–ä¿¡å·é‡ï¼ˆå¹¶å‘æ§åˆ¶ï¼‰
        logger.info(f"[{request_id}] ç­‰å¾…è·å–ä¿¡å·é‡ - å¯ç”¨æ•°é‡: {workflow_semaphore._value}")
        async with workflow_semaphore:
            logger.info(f"[{request_id}] å·²è·å–ä¿¡å·é‡ï¼Œå¼€å§‹å¤„ç†")

            # 2. è¶…æ—¶ä¿æŠ¤ - ä½¿ç”¨ç®€åŒ–çš„è¶…æ—¶å¤„ç†
            try:
                # è®°å½•å¼€å§‹æ—¶é—´ç”¨äºè¶…æ—¶æ£€æµ‹
                start_time = time.time()

                # æ‰§è¡Œå·¥ä½œæµç”Ÿæˆå™¨
                async for chunk in execute_workflow_stream_internal(request_data, request_id):
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    if time.time() - start_time > WORKFLOW_TIMEOUT:
                        logger.error(f"[{request_id}] å·¥ä½œæµæ‰§è¡Œè¶…æ—¶")
                        raise asyncio.TimeoutError("å·¥ä½œæµæ‰§è¡Œè¶…æ—¶")

                    yield chunk

                # æˆåŠŸå®Œæˆ
                duration = time.time() - request_start_time
                update_metrics(success=True, duration=duration)
                logger.info(f"[{request_id}] è¯·æ±‚æˆåŠŸå®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")

            except asyncio.TimeoutError:
                # è¶…æ—¶å¤„ç†
                duration = time.time() - request_start_time
                update_metrics(success=False, duration=duration, timeout=True)
                logger.error(f"[{request_id}] è¯·æ±‚è¶…æ—¶ï¼Œè€—æ—¶: {duration:.2f}s")

                yield f"{json.dumps({'type': 'error', 'message': f'â° å¤„ç†è¶…æ—¶ï¼ˆ{WORKFLOW_TIMEOUT}ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•', 'progress': 0}, ensure_ascii=False)}\n"
                yield f"{json.dumps({'type': 'done'}, ensure_ascii=False)}\n"

    except Exception as e:
        # å…¶ä»–å¼‚å¸¸å¤„ç†
        duration = time.time() - request_start_time
        update_metrics(success=False, duration=duration)
        logger.error(f"[{request_id}] è¯·æ±‚å¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)

        yield f"{json.dumps({'type': 'error', 'message': f'âŒ å¤„ç†å¤±è´¥: {str(e)}', 'progress': 0}, ensure_ascii=False)}\n"
        yield f"{json.dumps({'type': 'done'}, ensure_ascii=False)}\n"

    finally:
        # å‡å°‘æ´»è·ƒè¯·æ±‚è®¡æ•°
        decrement_active_requests()
        logger.info(f"[{request_id}] è¯·æ±‚å¤„ç†å®Œæˆ - å½“å‰æ´»è·ƒè¯·æ±‚: {performance_metrics.active_requests}")

        # è®°å½•è¯¦ç»†çš„æ€§èƒ½æ—¥å¿—
        perf_snapshot = get_performance_snapshot()
        logger.info(f"[{request_id}] æ€§èƒ½å¿«ç…§ - æˆåŠŸç‡: {perf_snapshot['success_rate']}, "
                   f"å¹³å‡å“åº”æ—¶é—´: {perf_snapshot['avg_response_time']}, "
                   f"å¯ç”¨ä¿¡å·é‡: {perf_snapshot['semaphore_available']}, "
                   f"é˜Ÿåˆ—å¤§å°: {perf_snapshot['queue_size']}")

async def execute_workflow_stream_internal(request_data: dict, request_id: str) -> AsyncGenerator[str, None]:
    """å†…éƒ¨å·¥ä½œæµæ‰§è¡Œé€»è¾‘ï¼ˆåŸexecute_workflow_streamçš„æ ¸å¿ƒé€»è¾‘ï¼‰"""
    try:
        request = PregnantWorkflowRequest(**request_data)
        start_time = time.time()

        # å‘é€å¼€å§‹æ¶ˆæ¯
        logger.info(f"[{request_id}] å‘é€å¼€å§‹æ¶ˆæ¯ - è¾“å…¥é•¿åº¦: {len(request.input)}")
        yield f"{json.dumps({'type': 'start', 'message': 'ğŸš€ å¼€å§‹å¤„ç†æ‚¨çš„é—®é¢˜...', 'timestamp': datetime.now().isoformat(), 'progress': 0}, ensure_ascii=False)}\n"

        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œè®©å‰ç«¯èƒ½çœ‹åˆ°å¼€å§‹æ¶ˆæ¯
        await asyncio.sleep(0.1)

        # å‘é€è¿›åº¦æ¶ˆæ¯ï¼ˆå¢åŠ æ›´è¯¦ç»†çš„ç›‘æ§ä¿¡æ¯ï¼‰
        perf_snapshot = get_performance_snapshot()
        logger.info(f"[{request_id}] å·¥ä½œæµåˆå§‹åŒ– - å½“å‰å¹¶å‘: {perf_snapshot['active_requests']}, "
                   f"å¯ç”¨ä¿¡å·é‡: {perf_snapshot['semaphore_available']}, "
                   f"é˜Ÿåˆ—å¤§å°: {perf_snapshot['queue_size']}")
        progress_msg = f"ğŸ“‹ æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½åŒ»ç–—å·¥ä½œæµ... (å½“å‰å¹¶å‘: {perf_snapshot['active_requests']})"
        yield f"{json.dumps({'type': 'progress', 'message': progress_msg, 'progress': 5}, ensure_ascii=False)}\n"

        # æ„é€ ç”¨æˆ·æ¶ˆæ¯
        user_message_id = f"msg_{uuid.uuid4()}"
        user_content: List[MessageContent] = [TextContent(type="text", text=request.input)]

        # å¤„ç†æ–‡ä»¶
        if request.file_id:
            yield f"{json.dumps({'type': 'progress', 'message': f'ğŸ“ æ­£åœ¨åˆ†ææ‚¨ä¸Šä¼ çš„ {len(request.file_id)} ä¸ªåŒ»ç–—æ–‡ä»¶...', 'progress': 15}, ensure_ascii=False)}\n"

            for i, file_id_str in enumerate(request.file_id):
                try:
                    file_info = maternal_service.get_medical_file_by_fileid(file_id_str)
                    if file_info:
                        file_name = file_info.get("file_name", "æœªçŸ¥æ–‡ä»¶")
                        file_type = file_info.get("file_type", "").lower()

                        yield f"{json.dumps({'type': 'progress', 'message': f'ğŸ“„ æ­£åœ¨è§£ææ–‡ä»¶: {file_name}', 'progress': 15 + (i+1) * 5}, ensure_ascii=False)}\n"

                        if (file_type.startswith("image/") or
                            file_type in ["jpg", "jpeg", "png", "gif", "bmp", "webp"]):
                            user_content.append(ImageUrlContent(
                                type="image_url",
                                image_url=ImageUrlInfo(file_id=file_id_str)
                            ))
                        else:
                            user_content.append(DocumentContent(
                                type="document",
                                document=DocumentInfo(file_id=file_id_str)
                            ))
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ID {file_id_str} æ—¶å‡ºé”™: {str(e)}")
                    yield f"{json.dumps({'type': 'progress', 'message': f'âš ï¸ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œç»§ç»­å¤„ç†å…¶ä»–å†…å®¹...', 'progress': 15 + (i+1) * 5}, ensure_ascii=False)}\n"
                    continue

        user_message = MessageItem(
            message_id=user_message_id,
            role="user",
            content=user_content,
            timestamp=request.timestamp
        )

        # æ‰§è¡Œå·¥ä½œæµ
        yield f"{json.dumps({'type': 'progress', 'message': 'ğŸ¤– æ­£åœ¨å¯åŠ¨AIæ™ºèƒ½è¯Šç–—ç³»ç»Ÿ...', 'progress': 35}, ensure_ascii=False)}\n"
        await asyncio.sleep(0.2)

        yield f"{json.dumps({'type': 'progress', 'message': 'ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³åŒ»ç–—çŸ¥è¯†åº“...', 'progress': 45}, ensure_ascii=False)}\n"
        await asyncio.sleep(0.3)

        yield f"{json.dumps({'type': 'progress', 'message': 'ğŸ§  AIåŒ»ç”Ÿæ­£åœ¨åˆ†ææ‚¨çš„æƒ…å†µ...', 'progress': 60}, ensure_ascii=False)}\n"

        # å‡†å¤‡å·¥ä½œæµæ‰§è¡Œ
        logger.info(f"[{request_id}] å¼€å§‹æ‰§è¡Œå·¥ä½œæµ - maternal_id: {request.maternal_id}, chat_id: {request.chat_id}")
        workflow_graph = prengant_workflow()
        workflow_state: PrengantState = {
            "input": request.input,
            "maternal_id": request.maternal_id,
            "chat_id": request.chat_id,
            "user_type": request.user_type,
            "timestamp": request.timestamp,
            "file_id": request.file_id or []
        }

        # æ‰§è¡Œæµå¼å·¥ä½œæµï¼ˆçœŸæ­£çš„AIæµå¼è¾“å‡ºï¼‰
        workflow_start_time = time.time()
        logger.info(f"[{request_id}] å¼€å§‹æµå¼å·¥ä½œæµ - å¼€å§‹æ—¶é—´: {workflow_start_time}")

        # å¼•å…¥æµå¼å·¥ä½œæµ
        from backend.workflow.test import prengant_workflow_stream

        # å‘é€AIå¼€å§‹ç”Ÿæˆçš„æ¶ˆæ¯
        yield f"{json.dumps({'type': 'progress', 'message': 'ğŸ¤– AIåŒ»ç”Ÿå¼€å§‹å›ç­”...', 'progress': 70}, ensure_ascii=False)}\n"

        # å®æ—¶æµå¼è¾“å‡ºAIç”Ÿæˆçš„å†…å®¹
        full_response = ""
        chunk_count = 0

        async for ai_chunk in prengant_workflow_stream(workflow_state):
            chunk_count += 1
            full_response += ai_chunk

            # å®æ—¶å‘é€AIç”Ÿæˆçš„æ¯ä¸ªchunkç»™å‰ç«¯
            yield f"{json.dumps({'type': 'ai_content', 'content': ai_chunk, 'chunk_id': chunk_count}, ensure_ascii=False)}\n"

            # å¯é€‰ï¼šæ¯éš”å‡ ä¸ªchunkå‘é€ä¸€æ¬¡è¿›åº¦æ›´æ–°
            if chunk_count % 10 == 0:
                progress = min(70 + (chunk_count // 10), 95)
                yield f"{json.dumps({'type': 'progress', 'message': f'ğŸ’­ AIæ­£åœ¨æ€è€ƒ... ({chunk_count} tokens)', 'progress': progress}, ensure_ascii=False)}\n"

        workflow_end_time = time.time()
        workflow_duration = workflow_end_time - workflow_start_time

        logger.info(f"[{request_id}] æµå¼å·¥ä½œæµå®Œæˆ - è€—æ—¶: {workflow_duration:.2f}s, chunks: {chunk_count}")

        # è®¾ç½®æœ€ç»ˆçš„AIå›å¤å†…å®¹
        workflow_output = full_response.strip() if full_response.strip() else None
        workflow_error = None if workflow_output else "AIæœªç”Ÿæˆæœ‰æ•ˆå›å¤"

        yield f"{json.dumps({'type': 'progress', 'message': f'âœ… AIå›å¤å®Œæˆï¼ˆè€—æ—¶ {workflow_duration:.1f}ç§’ï¼Œå…± {chunk_count} tokensï¼‰', 'progress': 95}, ensure_ascii=False)}\n"

        # æ„é€ åŠ©æ‰‹æ¶ˆæ¯
        assistant_message: Optional[MessageItem] = None
        if workflow_output:
            assistant_message_id = f"msg_{uuid.uuid4()}"
            assistant_content: List[MessageContent] = [TextContent(type="text", text=workflow_output)]
            user_dt = datetime.strptime(request.timestamp, "%Y-%m-%dT%H:%M:%S.%fZ")
            assistant_dt = user_dt + timedelta(seconds=int(workflow_duration))
            assistant_message = MessageItem(
                message_id=assistant_message_id,
                role="assistant",
                content=assistant_content,
                timestamp=assistant_dt.strftime("%Y-%m-%dT%H:%M:%S.%fZ")
            )

        # æ„é€ ä¼šè¯æ ‡é¢˜
        session_title = request.input[:10] + "..." if len(request.input) > 10 else request.input
        if not session_title:
            session_title = "æœªå‘½åä¼šè¯"

        # æ„é€ å“åº”æ•°æ®
        workflow_data = WorkflowData(
            chat_meta=ChatMeta(
                chat_id=request.chat_id,
                user_type=request.user_type,
                maternal_id=request.maternal_id
            ),
            session_title=session_title,
            messages=[user_message] + ([assistant_message] if assistant_message else []),
            error=workflow_error
        )

        answer = PregnantWorkflowResponse(
            code=200 if workflow_output else 500,
            msg="success" if workflow_output else "å·¥ä½œæµæœªç”Ÿæˆæœ‰æ•ˆå›ç­”",
            data=workflow_data
        )

        # ä¿å­˜ç»“æœ
        if workflow_output:
            try:
                yield f"{json.dumps({'type': 'progress', 'message': 'ğŸ’¾ æ­£åœ¨ä¿å­˜å¯¹è¯è®°å½•...', 'progress': 95}, ensure_ascii=False)}\n"

                json_file_path = maternal_service.get_dialogue_content_by_chat_id(request.chat_id)
                if isinstance(json_file_path, str):
                    os.makedirs(os.path.dirname(json_file_path), exist_ok=True)

                    existing_data = []
                    if os.path.exists(json_file_path):
                        try:
                            with open(json_file_path, 'r', encoding='utf-8') as f:
                                existing_data = json.load(f)
                                if not isinstance(existing_data, list):
                                    existing_data = [existing_data]
                        except json.JSONDecodeError:
                            existing_data = []

                    existing_data.append(answer.model_dump())
                    with open(json_file_path, 'w', encoding='utf-8') as f:
                        json.dump(existing_data, f, ensure_ascii=False, indent=2)

                    logger.info(f"ç­”æ¡ˆå·²ä¿å­˜åˆ°JSONæ–‡ä»¶: {json_file_path}")
            except Exception as e:
                logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
                yield f"{json.dumps({'type': 'warning', 'message': 'âš ï¸ å¯¹è¯è®°å½•ä¿å­˜å¤±è´¥ï¼Œä½†ç»“æœå·²ç”Ÿæˆ', 'progress': 95}, ensure_ascii=False)}\n"

        # è®¡ç®—æ€»è€—æ—¶
        total_duration = time.time() - start_time

        # å‘é€æœ€ç»ˆç»“æœ
        yield f"{json.dumps({'type': 'complete', 'message': f'ğŸ‰ å¤„ç†å®Œæˆï¼æ€»è€—æ—¶ {total_duration:.1f}ç§’', 'data': answer.model_dump(), 'progress': 100, 'duration': total_duration}, ensure_ascii=False)}\n"

        # å…³é—­è¿æ¥
        yield f"{json.dumps({'type': 'done'}, ensure_ascii=False)}\n"

    except Exception as e:
        error_msg = f"å·¥ä½œæµæ‰§è¡Œå¼‚å¸¸ï¼š{str(e)}"
        logger.error(error_msg, exc_info=True)

        # å‘é€é”™è¯¯æ¶ˆæ¯
        yield f"{json.dumps({'type': 'error', 'message': f'âŒ å¤„ç†å¤±è´¥: {error_msg}', 'progress': 0}, ensure_ascii=False)}\n"
        yield f"{json.dumps({'type': 'done'}, ensure_ascii=False)}\n"

# ------------------------------
# 4. å·¥ä½œæµè°ƒç”¨æ¥å£å®ç°
# ------------------------------

@router.post(
    "/qa",
    summary="å­•å¦‡å·¥ä½œæµè°ƒç”¨æ¥å£ï¼ˆæµå¼è¿”å›ï¼‰",
    description="""
    è°ƒç”¨å­•å¦‡ä¸“å±æ™ºèƒ½å·¥ä½œæµï¼Œæ”¯æŒå®æ—¶æµå¼è¿”å›ï¼š

    **åŠŸèƒ½ç‰¹æ€§**ï¼š
    1. ç«‹å³å¼€å§‹æµå¼å“åº”ï¼Œå®æ—¶æ¨é€å¤„ç†è¿›åº¦
    2. é€šè¿‡Server-Sent Eventsè¿”å›è¿›åº¦å’Œæœ€ç»ˆç»“æœ
    3. ç”¨æˆ·å¯å®æ—¶çœ‹åˆ°å¤„ç†çŠ¶æ€ï¼Œæ— éœ€ç­‰å¾…
    4. è¿æ¥ç»“æŸåç›´æ¥è·å¾—å®Œæ•´å¯¹è¯æ•°æ®
    5. æ”¯æŒå¤šç”¨æˆ·å¹¶å‘ï¼ˆ20ä¸ªå¹¶å‘ + 50ä¸ªé˜Ÿåˆ—ï¼‰

    **å“åº”æ ¼å¼ï¼ˆServer-Sent Eventsï¼‰**ï¼š
    - type: start - å¤„ç†å¼€å§‹
    - type: progress - å¤„ç†è¿›åº¦æ›´æ–°ï¼ˆå«è¿›åº¦ç™¾åˆ†æ¯”ï¼‰
    - type: ai_content - AIå®æ—¶ç”Ÿæˆå†…å®¹
    - type: complete - å¤„ç†å®Œæˆï¼ŒåŒ…å«å®Œæ•´ç»“æœ
    - type: error - å¤„ç†å¤±è´¥
    """,
    status_code=status.HTTP_200_OK,
    responses={
        200: {
            "description": "JSON Lines æµå¼å“åº”",
            "content": {
                "application/x-ndjson": {
                    "example": "{\"type\":\"start\",\"message\":\"ğŸš€ å¼€å§‹å¤„ç†æ‚¨çš„é—®é¢˜...\"}\n{\"type\":\"ai_content\",\"content\":\"æ‚¨å¥½\",\"chunk_id\":1}\n{\"type\":\"done\"}\n"
                }
            }
        }
    }
)
async def invoke_pregnant_workflow_stream(
    request: PregnantWorkflowRequest,
    use_queue: bool = Query(False, description="æ˜¯å¦ä½¿ç”¨è¯·æ±‚é˜Ÿåˆ—ï¼ˆé«˜è´Ÿè½½æ—¶æ¨èå¼€å¯ï¼‰")
):
    """è°ƒç”¨å­•å¦‡å·¥ä½œæµï¼ˆæµå¼è¿”å›ï¼‰"""
    try:
        # ç¡®ä¿é˜Ÿåˆ—å¤„ç†å™¨å·²å¯åŠ¨
        await ensure_queue_processor_started()

        logger.info(f"å¼€å§‹æµå¼å·¥ä½œæµï¼šmaternal_id={request.maternal_id}, chat_id={request.chat_id}, use_queue={use_queue}")

        if use_queue:
            # ä½¿ç”¨é˜Ÿåˆ—æ¨¡å¼å¤„ç†
            if request_queue.full():
                # é˜Ÿåˆ—å·²æ»¡ï¼Œè¿”å›é”™è¯¯
                logger.warning(f"è¯·æ±‚é˜Ÿåˆ—å·²æ»¡ï¼Œæ‹’ç»è¯·æ±‚ï¼šmaternal_id={request.maternal_id}")
                raise HTTPException(
                    status_code=503,
                    detail="æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•"
                )

            # å°†è¯·æ±‚æ”¾å…¥é˜Ÿåˆ—
            response_future = asyncio.Future()
            await request_queue.put((request.model_dump(), response_future))
            update_queue_waiting(request_queue.qsize())

            logger.info(f"è¯·æ±‚å·²åŠ å…¥é˜Ÿåˆ—ï¼šmaternal_id={request.maternal_id}, é˜Ÿåˆ—å¤§å°={request_queue.qsize()}")

            # ç­‰å¾…é˜Ÿåˆ—å¤„ç†ç»“æœå¹¶è¿”å›æµå¼å“åº”
            return StreamingResponse(
                wait_for_queue_result(response_future),
                media_type="application/x-ndjson",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "Cache-Control"
                }
            )
        else:
            # ç›´æ¥å¤„ç†æ¨¡å¼
            return StreamingResponse(
                execute_workflow_stream_protected(request.model_dump()),
                media_type="application/x-ndjson",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Headers": "Cache-Control"
                }
            )

    except Exception as e:
        logger.error(f"å·¥ä½œæµè°ƒç”¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨æµå¼å·¥ä½œæµå¤±è´¥: {str(e)}")

# ------------------------------
# 7. æ€§èƒ½ç›‘æ§æ¥å£
# ------------------------------

# @router.get(
#     "/qa/performance",
#     summary="è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡",
#     description="è·å–å½“å‰ç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å¹¶å‘æƒ…å†µã€å“åº”æ—¶é—´ã€æˆåŠŸç‡ç­‰",
#     status_code=status.HTTP_200_OK
# )
# async def get_performance_metrics():
#     """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
#     try:
#         perf_snapshot = get_performance_snapshot()

#         return {
#             "code": 200,
#             "msg": "è·å–æ€§èƒ½æŒ‡æ ‡æˆåŠŸ",
#             "data": {
#                 "timestamp": datetime.now().isoformat(),
#                 "performance": perf_snapshot,
#                 "system_config": {
#                     "max_concurrent_requests": MAX_CONCURRENT_REQUESTS,
#                     "workflow_timeout": WORKFLOW_TIMEOUT,
#                     "queue_max_size": QUEUE_MAX_SIZE
#                 },
#                 "recommendations": generate_performance_recommendations(perf_snapshot)
#             }
#         }
#     except Exception as e:
#         logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
#         return {
#             "code": 500,
#             "msg": f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {str(e)}",
#             "data": None
#         }

# def generate_performance_recommendations(perf_snapshot: dict) -> List[str]:
#     """æ ¹æ®æ€§èƒ½æŒ‡æ ‡ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
#     recommendations = []

#     # æ£€æŸ¥æˆåŠŸç‡
#     success_rate = float(perf_snapshot["success_rate"].rstrip('%'))
#     if success_rate < 90:
#         recommendations.append("ğŸš¨ æˆåŠŸç‡ä½äº90%ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿè´Ÿè½½å’Œé”™è¯¯æ—¥å¿—")

#     # æ£€æŸ¥å¹¶å‘æƒ…å†µ
#     active_requests = perf_snapshot["active_requests"]
#     semaphore_available = perf_snapshot["semaphore_available"]
#     if semaphore_available <= 2:
#         recommendations.append("âš ï¸ å¹¶å‘èµ„æºç´§å¼ ï¼Œå»ºè®®å¢åŠ MAX_CONCURRENT_REQUESTSæˆ–ä½¿ç”¨é˜Ÿåˆ—æ¨¡å¼")

#     # æ£€æŸ¥é˜Ÿåˆ—æƒ…å†µ
#     queue_size = perf_snapshot["queue_size"]
#     if queue_size > QUEUE_MAX_SIZE * 0.8:
#         recommendations.append("ğŸ“‹ è¯·æ±‚é˜Ÿåˆ—æ¥è¿‘æ»¡è½½ï¼Œå»ºè®®å¢åŠ å¤„ç†èƒ½åŠ›æˆ–queueå¤§å°")

#     # æ£€æŸ¥å“åº”æ—¶é—´
#     avg_response_time = float(perf_snapshot["avg_response_time"].rstrip('s'))
#     if avg_response_time > 60:
#         recommendations.append("â° å¹³å‡å“åº”æ—¶é—´è¶…è¿‡60ç§’ï¼Œå»ºè®®ä¼˜åŒ–å·¥ä½œæµæ€§èƒ½")

#     # æ£€æŸ¥è¶…æ—¶ç‡
#     total_requests = perf_snapshot["total_requests"]
#     timeout_requests = perf_snapshot["timeout_requests"]
#     if total_requests > 0:
#         timeout_rate = (timeout_requests / total_requests) * 100
#         if timeout_rate > 10:
#             recommendations.append("â±ï¸ è¶…æ—¶ç‡è¶…è¿‡10%ï¼Œå»ºè®®å¢åŠ WORKFLOW_TIMEOUTæˆ–ä¼˜åŒ–å¤„ç†é€»è¾‘")

#     if not recommendations:
#         recommendations.append("âœ… ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½")

#     return recommendations

# @router.get(
#     "/health",
#     summary="ç³»ç»Ÿå¥åº·æ£€æŸ¥",
#     description="æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€ï¼ŒåŒ…æ‹¬æœåŠ¡å¯ç”¨æ€§å’Œå…³é”®æŒ‡æ ‡",
#     status_code=status.HTTP_200_OK
# )
# async def health_check():
#     """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
#     try:
#         # æ£€æŸ¥åŸºæœ¬æœåŠ¡çŠ¶æ€
#         perf_snapshot = get_performance_snapshot()

#         # åˆ¤æ–­ç³»ç»Ÿå¥åº·çŠ¶æ€
#         is_healthy = True
#         health_issues = []

#         # æ£€æŸ¥æˆåŠŸç‡
#         success_rate = float(perf_snapshot["success_rate"].rstrip('%'))
#         if success_rate < 80:
#             is_healthy = False
#             health_issues.append("æˆåŠŸç‡è¿‡ä½")

#         # æ£€æŸ¥å“åº”æ—¶é—´
#         avg_response_time = float(perf_snapshot["avg_response_time"].rstrip('s'))
#         if avg_response_time > 120:
#             is_healthy = False
#             health_issues.append("å“åº”æ—¶é—´è¿‡é•¿")

#         # æ£€æŸ¥ä¿¡å·é‡å¯ç”¨æ€§
#         if perf_snapshot["semaphore_available"] <= 0:
#             is_healthy = False
#             health_issues.append("æ— å¯ç”¨å¹¶å‘èµ„æº")

#         # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
#         if perf_snapshot["queue_size"] >= QUEUE_MAX_SIZE:
#             is_healthy = False
#             health_issues.append("è¯·æ±‚é˜Ÿåˆ—å·²æ»¡")

#         return {
#             "status": "healthy" if is_healthy else "unhealthy",
#             "timestamp": datetime.now().isoformat(),
#             "version": "v2.0.0",
#             "uptime": "unknown",  # å¯ä»¥æ·»åŠ å¯åŠ¨æ—¶é—´è®°å½•
#             "performance": perf_snapshot,
#             "issues": health_issues if health_issues else None
#         }

#     except Exception as e:
#         logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
#         return {
#             "status": "error",
#             "timestamp": datetime.now().isoformat(),
#             "error": str(e)
#         }


# ------------------------------
# 2. æ ¸å¿ƒæ¥å£å®ç°ï¼ˆæ ¹æ® chat_id è·å–å¯¹è¯å†å²ï¼‰
# ------------------------------


@router.get(
    path = '/{chat_id}/history',
    summary = 'æ ¹æ®',
    description = 'æ ¹æ®å¯¹è¯IDè·å–å¯¹è¯å†å²'
)
async def get_chat_history_by_ids(
    chat_id: str = Path(..., description="å¯¹è¯ID")
):
    json_file_path = maternal_service.get_dialogue_content_by_chat_id(chat_id)
    if not isinstance(json_file_path, str):
        raise HTTPException(status_code=404, detail="å¯¹è¯æ–‡ä»¶ä¸å­˜åœ¨")
    with open(json_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data

# ------------------------------
# 2. æ ¸å¿ƒæ¥å£å®ç°ï¼ˆæ ¹æ® chat_id è·å–å¯¹è¯å†å²ï¼‰
# ------------------------------
class GetChatIdsRequest(BaseModel):
    maternal_id: int = Field(..., description="å­•å¦‡ID")


@router.post(
    path = '/get_chat_ids',
    summary = 'åŸæœ¬è·¯ç”±æ ¹æ®å­•å¦‡IDè·å–å¯¹è¯IDåˆ—è¡¨',
    description = 'æ ¹æ®å­•å¦‡IDè·å–æ‰€æœ‰å¯¹è¯è®°å½•çš„å¯¹è¯IDåˆ—è¡¨'
)
async def get_chat_ids_by_maternal_id(
    request: GetChatIdsRequest
):
    chat_ids = maternal_service.get_chat_id_by_maternal_id(request.maternal_id)
    return chat_ids


# ------------------------------
# 6. åŒ»ç–—æ–‡ä»¶ç›¸å…³æ¥å£
# ------------------------------
@router.post(
    path="/{user_id}/files",
    status_code=status.HTTP_201_CREATED,
    summary="ä¸Šä¼ å­•å¦‡åŒ»ç–—æ–‡ä»¶",
    description="ä¸Šä¼ å­•å¦‡åŒ»ç–—æ–‡ä»¶ï¼ˆæ”¯æŒjpg/png/pdfç­‰ï¼Œéœ€form-dataæ ¼å¼ï¼‰"
)
# @require_auth  # å¦‚éœ€è®¤è¯å¯å–æ¶ˆæ³¨é‡Š
def upload_medical_file(
    user_id: int = Path(..., description="å­•å¦‡å”¯ä¸€IDï¼ˆæ­£æ•´æ•°ï¼‰"),
    # æ–‡ä»¶å‚æ•°ï¼šFastAPIåŸç”ŸUploadFileï¼Œè‡ªåŠ¨å¤„ç†æ–‡ä»¶æµ
    file: UploadFile = File(..., description="ä¸Šä¼ çš„åŒ»ç–—æ–‡ä»¶ï¼ˆå¿…å¡«ï¼‰"),
    # è¡¨å•å‚æ•°ï¼šç”¨Formæ ‡æ³¨ï¼ˆform-dataä¸­çš„éæ–‡ä»¶å­—æ®µï¼‰
    file_desc: str | None = Form(None, description="æ–‡ä»¶æè¿°ï¼ˆå¯é€‰ï¼‰"),
    check_date_str: str | None = Form(None, description="æ£€æŸ¥æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼Œå¯é€‰ï¼‰")
):
    try:
        # 1. éªŒè¯æ–‡ä»¶åˆæ³•æ€§
        if not file.filename:
            return JSONResponse(
                content={"status": "error", "message": "æœªé€‰æ‹©æ–‡ä»¶æˆ–æ–‡ä»¶åä¸ºç©º"},
                status_code=status.HTTP_400_BAD_REQUEST
            )

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_content = file.file.read()
        max_size = 10 * 1024 * 1024  # 10MB
        if len(file_content) > max_size:
            return JSONResponse(
                content={"status": "error", "message": "æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡10MB"},
                status_code=status.HTTP_400_BAD_REQUEST
            )

        # 2. å¤„ç†æ–‡ä»¶å­˜å‚¨ç›®å½•
        base_upload_dir = "uploads/maternal_files"
        user_dir = os.path.join(base_upload_dir, str(user_id))
        os.makedirs(user_dir, exist_ok=True)  # ä¸å­˜åœ¨åˆ™åˆ›å»ºç›®å½•

        # 3. ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼ˆé¿å…å†²çªï¼‰
        file_ext = os.path.splitext(file.filename)[1].lower()  # æå–æ–‡ä»¶åç¼€ï¼ˆå°å†™ï¼‰
        unique_filename = f"{uuid.uuid4()}{file_ext}"  # UUIDç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_path = os.path.join(user_dir, unique_filename)

        # 4. ä¿å­˜æ–‡ä»¶åˆ°æœåŠ¡å™¨ï¼ˆFastAPI UploadFile éœ€ç”¨æ–‡ä»¶å¯¹è±¡ä¿å­˜ï¼‰
        with open(file_path, "wb") as f:
            f.write(file_content)  # è¯»å–ä¸Šä¼ æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹å¹¶å†™å…¥

        # 5. å¤„ç†æ£€æŸ¥æ—¥æœŸ
        check_date = None
        if check_date_str:
            check_date = datetime.strptime(check_date_str, "%Y-%m-%d").date()

        # 6. è·å–æ–‡ä»¶å…ƒä¿¡æ¯
        file_size = os.path.getsize(file_path)  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        file_type = file.content_type or file_ext.lstrip(".")  # ä¼˜å…ˆç”¨MIMEç±»å‹ï¼Œå…¶æ¬¡åç¼€

        # 7. è°ƒç”¨ä¸šåŠ¡å±‚ä¿å­˜åˆ°æ•°æ®åº“
        db_result = maternal_service.create_medical_file(
            maternal_id=user_id,
            file_name=file.filename,  # åŸå§‹æ–‡ä»¶å
            file_path=file_path,      # æœåŠ¡å™¨å­˜å‚¨è·¯å¾„
            file_type=file_type,
            file_size=file_size,
            upload_time=datetime.now(),
            file_desc=file_desc,
            check_date=check_date
        )

        # 8. æ„é€ å“åº”ï¼ˆè¿”å›å…³é”®ä¿¡æ¯ï¼‰
        file_id = None
        if hasattr(db_result, "id"):
            file_id = getattr(db_result, "id")
        elif isinstance(db_result, dict) and "id" in db_result:
            file_id = db_result["id"]
        
        return {
            "status": "success",
            "message": "åŒ»ç–—æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            "data": {
                "file_id": file_id,
                "original_filename": file.filename,
                "storage_path": file_path,
                "file_type": file_type,
                "file_size": file_size,
                "upload_time": datetime.now().strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
                "check_date": check_date.strftime("%Y-%m-%d") if check_date else None,
                "file_desc": file_desc
            }
        }

    except Exception as e:
        # å‡ºé”™æ—¶æ¸…ç†å·²ä¿å­˜çš„æ–‡ä»¶ï¼ˆé¿å…åƒåœ¾æ–‡ä»¶ï¼‰
        file_path_local = locals().get("file_path")
        if file_path_local and os.path.exists(file_path_local):
            try:
                os.remove(file_path_local)
            except OSError:
                pass  # å¿½ç•¥åˆ é™¤æ–‡ä»¶æ—¶çš„é”™è¯¯
        
        return JSONResponse(
            content={"status": "error", "message": f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼š{str(e)}"},
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
    finally:
        # å…³é—­æ–‡ä»¶æµï¼ˆé¿å…èµ„æºæ³„æ¼ï¼‰
        file.file.close()

class GetMedicalFilesRequest(BaseModel):
    file_id: str = Field(..., description="å­•å¦‡ID")

@router.get(
    path="/{user_id}/files",
    status_code=status.HTTP_200_OK,
    summary="æ ¹æ®file_idè·å–å­•å¦‡åŒ»ç–—æ–‡ä»¶ä¿¡æ¯",
    description="æ ¹æ®file_idè·å–å­•å¦‡åŒ»ç–—æ–‡ä»¶ä¿¡æ¯"
)
# @require_auth  # å¦‚éœ€è®¤è¯å¯å–æ¶ˆæ³¨é‡Š
def get_medical_files(
    request: GetMedicalFilesRequest
):
    try:
        file_records = maternal_service.get_medical_file_by_fileid(request.file_id)
        
        return {
            "status": "success",
            "count": len(file_records),
            "data": file_records
        }
    except Exception as e:
        return JSONResponse(
            content={"status": "error", "message": f"è·å–æ–‡ä»¶è®°å½•å¤±è´¥ï¼š{str(e)}"},
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@router.get(
    path="/{user_id}/files/{file_id}/download",
    status_code=status.HTTP_200_OK,
    summary="ä¸‹è½½å­•å¦‡åŒ»ç–—æ–‡ä»¶",
    description="æ ¹æ®ç”¨æˆ·IDå’Œæ–‡ä»¶IDä¸‹è½½æŒ‡å®šçš„åŒ»ç–—æ–‡ä»¶"
)
# @require_auth  # å¦‚éœ€è®¤è¯å¯å–æ¶ˆæ³¨é‡Š
def download_medical_file(
    user_id: int = Path(..., description="å­•å¦‡å”¯ä¸€IDï¼ˆæ­£æ•´æ•°ï¼‰"),
    file_id: str = Path(..., description="æ–‡ä»¶å”¯ä¸€IDï¼ˆæ­£æ•´æ•°ï¼‰")
):
    """ä¸‹è½½åŒ»ç–—æ–‡ä»¶"""
    try:
        # 1. è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = maternal_service.get_medical_file_by_fileid(file_id)
        
        if not file_info:
            return JSONResponse(
                content={"status": "error", "message": "æ–‡ä»¶ä¸å­˜åœ¨"},
                status_code=status.HTTP_404_NOT_FOUND
            )
        
        # 2. éªŒè¯æ–‡ä»¶å½’å±æƒï¼ˆç¡®ä¿æ–‡ä»¶å±äºæŒ‡å®šçš„å­•å¦‡ï¼‰
        if file_info["maternal_id"] != user_id:
            return JSONResponse(
                content={"status": "error", "message": "æ— æƒè®¿é—®è¯¥æ–‡ä»¶"},
                status_code=status.HTTP_403_FORBIDDEN
            )
        
        # 3. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºæœåŠ¡å™¨
        file_path = file_info["file_path"]
        
        # 4. å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        # ç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨å…è®¸çš„ä¸Šä¼ ç›®å½•èŒƒå›´å†…
        allowed_base_dir = os.path.abspath("uploads/maternal_files")
        actual_file_path = os.path.abspath(file_path)
        
        if not actual_file_path.startswith(allowed_base_dir):
            return JSONResponse(
                content={"status": "error", "message": "éæ³•æ–‡ä»¶è·¯å¾„"},
                status_code=status.HTTP_403_FORBIDDEN
            )
        
        if not os.path.exists(file_path):
            return JSONResponse(
                content={"status": "error", "message": "æ–‡ä»¶åœ¨æœåŠ¡å™¨ä¸Šä¸å­˜åœ¨"},
                status_code=status.HTTP_404_NOT_FOUND
            )
        
        # 5. ç¡®å®šæ–‡ä»¶çš„MIMEç±»å‹
        mime_type, _ = mimetypes.guess_type(file_path)
        if mime_type is None:
            mime_type = "application/octet-stream"  # é»˜è®¤äºŒè¿›åˆ¶ç±»å‹
        
        # 6. è¿”å›æ–‡ä»¶å“åº”
        return FileResponse(
            path=file_path,
            media_type=mime_type,
            filename=file_info["file_name"],  # ä½¿ç”¨åŸå§‹æ–‡ä»¶å
            headers={
                "Content-Disposition": f"attachment; filename*=UTF-8''{file_info['file_name']}",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache"
            }
        )
        
    except Exception as e:
        logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
        return JSONResponse(
            content={"status": "error", "message": f"ä¸‹è½½æ–‡ä»¶å¤±è´¥ï¼š{str(e)}"},
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@router.get(
    path="/{user_id}/files/list",
    status_code=status.HTTP_200_OK,
    summary="è·å–ç”¨æˆ·çš„æ‰€æœ‰åŒ»ç–—æ–‡ä»¶åˆ—è¡¨",
    description="æ ¹æ®ç”¨æˆ·IDè·å–è¯¥ç”¨æˆ·ä¸Šä¼ çš„æ‰€æœ‰åŒ»ç–—æ–‡ä»¶åˆ—è¡¨"
)
# @require_auth  # å¦‚éœ€è®¤è¯å¯å–æ¶ˆæ³¨é‡Š
def list_medical_files(
    user_id: int = Path(..., description="å­•å¦‡å”¯ä¸€IDï¼ˆæ­£æ•´æ•°ï¼‰"),
    file_name: Optional[str] = Query(None, description="å¯é€‰çš„æ–‡ä»¶åè¿‡æ»¤æ¡ä»¶")
):
    """è·å–ç”¨æˆ·çš„åŒ»ç–—æ–‡ä»¶åˆ—è¡¨"""
    try:
        # è·å–æ–‡ä»¶åˆ—è¡¨
        file_records = maternal_service.get_medical_files(user_id, file_name or "")
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶æ·»åŠ ä¸‹è½½é“¾æ¥
        for record in file_records:
            # æ·»åŠ ä¸‹è½½URLï¼ˆå‰ç«¯å¯ä»¥é€šè¿‡æ­¤URLç›´æ¥ä¸‹è½½æ–‡ä»¶ï¼‰
            record["download_url"] = f"/api/v2/chat/{user_id}/files/{record['id']}/download"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨äºæœåŠ¡å™¨ä¸Š
            record["file_exists"] = os.path.exists(record["file_path"])
        
        return {
            "status": "success",
            "count": len(file_records),
            "data": file_records
        }
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return JSONResponse(
            content={"status": "error", "message": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼š{str(e)}"},
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR
        )
