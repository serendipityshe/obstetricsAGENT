"""
å¼‚æ­¥åŒ–å·¥ä½œæµ - ä¿æŒä¸“å®¶çŸ¥è¯†åº“è´¨é‡ï¼Œè§£å†³å¤šç”¨æˆ·å¹¶å‘é˜»å¡
å°†åŒæ­¥æ£€ç´¢æ“ä½œæ”¹ä¸ºå¼‚æ­¥ï¼Œæ”¯æŒçœŸæ­£çš„é«˜å¹¶å‘
"""
import asyncio
import time
import json
import uuid
from datetime import datetime
from typing import AsyncGenerator, Dict, Any
import logging
from concurrent.futures import ThreadPoolExecutor

from backend.workflow.test import PrengantState
from backend.api.v1.services.maternal_service import MaternalService

logger = logging.getLogger(__name__)

# ä¸“ç”¨çº¿ç¨‹æ± ç”¨äºæ£€ç´¢æ“ä½œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
RETRIEVAL_THREAD_POOL = ThreadPoolExecutor(
    max_workers=20,  # æ”¯æŒ20ä¸ªå¹¶å‘æ£€ç´¢
    thread_name_prefix="retrieval_"
)

class AsyncWorkflowWithRetrieval:
    """å¼‚æ­¥åŒ–å·¥ä½œæµå¼•æ“ - ä¿æŒæ£€ç´¢è´¨é‡ï¼Œè§£å†³å¹¶å‘é˜»å¡"""

    def __init__(self):
        self.maternal_service = MaternalService()

    async def execute_async_workflow_stream(self, state: PrengantState) -> AsyncGenerator[str, None]:
        """
        å¼‚æ­¥åŒ–å·¥ä½œæµæµå¼æ‰§è¡Œ
        ç­–ç•¥ï¼šæ‰€æœ‰æ£€ç´¢æ“ä½œå¼‚æ­¥åŒ–ï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯ï¼Œæ”¯æŒçœŸæ­£é«˜å¹¶å‘
        """
        start_time = time.time()
        request_id = f"req_{uuid.uuid4().hex[:8]}"

        try:
            # ç«‹å³å‘é€å¼€å§‹æ¶ˆæ¯
            yield self._create_message("start", "ğŸš€ å¼€å§‹æ™ºèƒ½è¯Šç–—åˆ†æ...", 0)

            # å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å¤„ç†æ­¥éª¤
            yield self._create_message("progress", "ğŸ” æ­£åœ¨æ£€ç´¢ä¸“å®¶çŸ¥è¯†åº“...", 10)

            # å¹¶è¡Œæ‰§è¡Œè®°å¿†ã€æ–‡ä»¶ã€æ£€ç´¢å¤„ç†ï¼ˆå…¨éƒ¨å¼‚æ­¥åŒ–ï¼‰
            memory_task = self._async_memory_processing(state)
            file_task = self._async_file_processing(state)
            retrieval_task = self._async_retrieval_processing(state)

            yield self._create_message("progress", "ğŸ§  AIåŒ»ç”Ÿæ­£åœ¨åˆ†æå¤šæºåŒ»ç–—çŸ¥è¯†...", 25)

            # ç­‰å¾…æ‰€æœ‰å¤„ç†å®Œæˆï¼Œä½†ä¸é˜»å¡å…¶ä»–ç”¨æˆ·
            memory_result, file_result, retrieval_result = await asyncio.gather(
                memory_task, file_task, retrieval_task, return_exceptions=True
            )

            yield self._create_message("progress", "ğŸ“š çŸ¥è¯†æ•´åˆå®Œæˆï¼Œå¼€å§‹ç”Ÿæˆä¸“ä¸šå»ºè®®...", 50)

            # å¤„ç†ç»“æœ
            workflow_state = state.copy()

            # æ•´åˆè®°å¿†ç»“æœ
            if not isinstance(memory_result, Exception):
                workflow_state.update(memory_result)

            # æ•´åˆæ–‡ä»¶ç»“æœ
            if not isinstance(file_result, Exception):
                workflow_state.update(file_result)

            # æ•´åˆæ£€ç´¢ç»“æœ
            if not isinstance(retrieval_result, Exception):
                workflow_state.update(retrieval_result)

            # ä¸Šä¸‹æ–‡å¤„ç†ï¼ˆå¼‚æ­¥åŒ–ï¼‰
            context_result = await self._async_context_processing(workflow_state)
            workflow_state.update(context_result)

            yield self._create_message("progress", "ğŸ¤– åŸºäºä¸“å®¶çŸ¥è¯†å¼€å§‹æµå¼ç”Ÿæˆ...", 70)

            # æµå¼ç”ŸæˆAIå›ç­”ï¼ˆåŸºäºå®Œæ•´çš„ä¸“å®¶çŸ¥è¯†ï¼‰
            chunk_count = 0
            full_response = ""

            async for ai_chunk in self._stream_ai_with_expert_knowledge(workflow_state, request_id):
                chunk_count += 1
                full_response += ai_chunk

                # å®æ—¶å‘é€AIå†…å®¹
                yield self._create_message("ai_content", "", 0, content=ai_chunk, chunk_id=chunk_count)

                # å®šæœŸå‘é€è¿›åº¦
                if chunk_count % 15 == 0:
                    progress = min(70 + chunk_count // 3, 95)
                    yield self._create_message("progress", f"ğŸ’­ åŸºäºä¸“å®¶çŸ¥è¯†ç”Ÿæˆä¸­... ({chunk_count} tokens)", progress)

            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - start_time
            yield self._create_message("progress", f"âœ… ä¸“å®¶çº§å›ç­”å®Œæˆ (è€—æ—¶ {total_time:.1f}ç§’ï¼Œå…± {chunk_count} tokens)", 95)

            # æ„é€ æœ€ç»ˆå“åº”æ•°æ®
            response_data = await self._build_response_data(state, full_response, total_time)
            yield self._create_message("complete", f"ğŸ‰ åŸºäºä¸“å®¶çŸ¥è¯†çš„è¯Šç–—å»ºè®®å·²å®Œæˆ", 100, data=response_data)
            yield self._create_message("done")

        except Exception as e:
            logger.error(f"[{request_id}] å¼‚æ­¥å·¥ä½œæµå¤±è´¥: {e}")
            yield self._create_message("error", f"âŒ å¤„ç†å¤±è´¥: {str(e)}")
            yield self._create_message("done")

    async def _async_memory_processing(self, state: PrengantState) -> Dict[str, Any]:
        """å¼‚æ­¥è®°å¿†å¤„ç† - ä¸é˜»å¡äº‹ä»¶å¾ªç¯"""
        def sync_memory_processing():
            from backend.workflow.test import memory_processing_node
            return memory_processing_node(state)

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(RETRIEVAL_THREAD_POOL, sync_memory_processing)

    async def _async_file_processing(self, state: PrengantState) -> Dict[str, Any]:
        """å¼‚æ­¥æ–‡ä»¶å¤„ç† - ä¸é˜»å¡äº‹ä»¶å¾ªç¯"""
        def sync_file_processing():
            from backend.workflow.test import mix_node
            return mix_node(state)

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(RETRIEVAL_THREAD_POOL, sync_file_processing)

    async def _async_retrieval_processing(self, state: PrengantState) -> Dict[str, Any]:
        """å¼‚æ­¥æ£€ç´¢å¤„ç† - æœ€é‡è¦çš„ä¼˜åŒ–ç‚¹"""
        def sync_retrieval_processing():
            from backend.workflow.test import retr_node
            return retr_node(state)

        loop = asyncio.get_event_loop()
        # ä½¿ç”¨ä¸“ç”¨çº¿ç¨‹æ± ï¼Œé¿å…é˜»å¡å…¶ä»–ç”¨æˆ·çš„è¯·æ±‚
        return await loop.run_in_executor(RETRIEVAL_THREAD_POOL, sync_retrieval_processing)

    async def _async_context_processing(self, state: PrengantState) -> Dict[str, Any]:
        """å¼‚æ­¥ä¸Šä¸‹æ–‡å¤„ç†"""
        def sync_context_processing():
            from backend.workflow.test import proc_context
            result_state = proc_context(state)
            # proc_contextè¿”å›çš„æ˜¯å®Œæ•´stateï¼Œæˆ‘ä»¬åªå–æ–°å¢çš„éƒ¨åˆ†
            return {"context": result_state.get("context", "")}

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(RETRIEVAL_THREAD_POOL, sync_context_processing)

    async def _stream_ai_with_expert_knowledge(self, state: PrengantState, request_id: str) -> AsyncGenerator[str, None]:
        """åŸºäºä¸“å®¶çŸ¥è¯†è¿›è¡Œæµå¼AIç”Ÿæˆ"""
        try:
            # æ„é€ å¢å¼ºçš„åŒ»ç–—æç¤ºè¯ï¼ˆåŒ…å«æ£€ç´¢åˆ°çš„ä¸“å®¶çŸ¥è¯†ï¼‰
            enhanced_prompt = self._create_expert_medical_prompt(state)

            # è·å–æ¨¡å‹é…ç½®
            model_config = await self._get_model_config()

            # æµå¼è°ƒç”¨AIï¼ˆåŸºäºå®Œæ•´çš„ä¸“å®¶çŸ¥è¯†ï¼‰
            from backend.agents.tools.tools import qwen_tool_stream

            async for ai_chunk in qwen_tool_stream(
                input=enhanced_prompt,
                img_path='',
                model_name=model_config["llm_model"],
                api_key=model_config["api_key"],
                base_url=model_config["base_url"],
                temperature=model_config["temperature"]
            ):
                yield ai_chunk

        except Exception as e:
            logger.error(f"[{request_id}] ä¸“å®¶çŸ¥è¯†AIç”Ÿæˆå¤±è´¥: {e}")
            yield f"\n\nâŒ AIç”Ÿæˆå¤±è´¥ï¼š{str(e)}"

    def _create_expert_medical_prompt(self, state: PrengantState) -> str:
        """åˆ›å»ºåŸºäºä¸“å®¶çŸ¥è¯†çš„åŒ»ç–—æç¤ºè¯"""
        user_input = state.get("input", "")
        user_type = state.get("user_type", "pregnant_mother")

        # è·å–æ£€ç´¢åˆ°çš„ä¸“å®¶çŸ¥è¯†
        retrieval_professor = state.get("retrieval_professor", [])
        retrieval_pregnant = state.get("retrieval_pregnant", [])
        compressed_memory = state.get("compressed_memory", "")
        file_content = state.get("file_content", "")
        context = state.get("context", "")

        # æ„å»ºå¢å¼ºæç¤ºè¯
        prompt_parts = [
            "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¦‡äº§ç§‘åŒ»ç”Ÿå’Œå­•æœŸå¥åº·é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹ä¸“å®¶çŸ¥è¯†åº“å’Œç”¨æˆ·ä¿¡æ¯ï¼Œä¸ºå­•å¦‡æä¾›å‡†ç¡®ã€æ¸©æš–ã€ä¸“ä¸šçš„åŒ»ç–—å»ºè®®ã€‚",
            f"\nç”¨æˆ·ç±»å‹ï¼š{user_type}",
            f"ç”¨æˆ·é—®é¢˜ï¼š{user_input}",
        ]

        # æ·»åŠ ä¸“å®¶çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
        if retrieval_professor:
            prompt_parts.append(f"\nã€ä¸“å®¶çŸ¥è¯†åº“ã€‘ï¼š\n{str(retrieval_professor[:3])}")  # å–å‰3æ¡æœ€ç›¸å…³

        # æ·»åŠ ä¸ªäººçŸ¥è¯†åº“æ£€ç´¢ç»“æœ
        if retrieval_pregnant:
            prompt_parts.append(f"\nã€ä¸ªäººåŒ»ç–—å†å²ã€‘ï¼š\n{str(retrieval_pregnant[:2])}")  # å–å‰2æ¡

        # æ·»åŠ å†å²å¯¹è¯è®°å¿†
        if compressed_memory:
            prompt_parts.append(f"\nã€å†å²å¯¹è¯æ‘˜è¦ã€‘ï¼š\n{compressed_memory[:300]}")

        # æ·»åŠ æ–‡ä»¶å†…å®¹
        if file_content:
            prompt_parts.append(f"\nã€ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶å†…å®¹ã€‘ï¼š\n{file_content[:200]}")

        # æ·»åŠ æ•´åˆçš„ä¸Šä¸‹æ–‡
        if context and context not in str(retrieval_professor):
            prompt_parts.append(f"\nã€è¡¥å……åŒ»ç–—ä¿¡æ¯ã€‘ï¼š\n{context[:300]}")

        prompt_parts.extend([
            "\nè¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š",
            "1. ä¼˜å…ˆåŸºäºä¸Šè¿°ä¸“å®¶çŸ¥è¯†åº“æä¾›å»ºè®®",
            "2. ä½¿ç”¨æ¸©æš–ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è°ƒ",
            "3. æä¾›åŸºäºå¾ªè¯åŒ»å­¦çš„å»ºè®®",
            "4. æ˜ç¡®æŒ‡å‡ºä½•æ—¶éœ€è¦å°±åŒ»",
            "5. ç»™å‡ºå…·ä½“å¯è¡Œçš„æ—¥å¸¸æŠ¤ç†å»ºè®®",
            "6. å¦‚æœæ¶‰åŠä¸¥é‡ç—‡çŠ¶ï¼ŒåŠ¡å¿…å»ºè®®ç«‹å³å°±åŒ»",
            "7. å›ç­”è¦å…¨é¢ä½†ç®€æ´ï¼Œé‡ç‚¹çªå‡º",
            "\nè¯·ç°åœ¨åŸºäºä¸“å®¶çŸ¥è¯†å¼€å§‹å›ç­”ï¼š"
        ])

        return "\n".join(prompt_parts)

    async def _get_model_config(self) -> Dict[str, Any]:
        """å¼‚æ­¥è·å–æ¨¡å‹é…ç½®"""
        def sync_get_config():
            import yaml
            with open("backend/config/model_settings.yaml", "r", encoding="utf-8") as f:
                model_settings = yaml.safe_load(f)
            return model_settings.get("DEFAULT_MODEL", {})

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, sync_get_config)

    async def _build_response_data(self, state: PrengantState, ai_response: str, duration: float) -> Dict[str, Any]:
        """æ„å»ºå“åº”æ•°æ®"""
        from backend.api.v2.routes.chat_routes import PregnantWorkflowRequest, MessageItem, TextContent, ChatMeta, WorkflowData, PregnantWorkflowResponse

        # æ„é€ ç”¨æˆ·æ¶ˆæ¯
        user_message_id = f"msg_{uuid.uuid4()}"
        user_content = [TextContent(type="text", text=state.get("input", ""))]
        user_message = MessageItem(
            message_id=user_message_id,
            role="user",
            content=user_content,
            timestamp=state.get("timestamp", datetime.now().isoformat())
        )

        # æ„é€ åŠ©æ‰‹æ¶ˆæ¯
        assistant_message_id = f"msg_{uuid.uuid4()}"
        assistant_content = [TextContent(type="text", text=ai_response)]
        assistant_message = MessageItem(
            message_id=assistant_message_id,
            role="assistant",
            content=assistant_content,
            timestamp=datetime.now().isoformat()
        )

        # æ„é€ ä¼šè¯æ ‡é¢˜
        session_title = state.get("input", "")[:20] + "..." if len(state.get("input", "")) > 20 else state.get("input", "")

        # æ„é€ å“åº”æ•°æ®
        workflow_data = WorkflowData(
            chat_meta=ChatMeta(
                chat_id=state.get("chat_id", ""),
                user_type=state.get("user_type", ""),
                maternal_id=state.get("maternal_id", 0)
            ),
            session_title=session_title,
            messages=[user_message, assistant_message],
            error=None
        )

        response = PregnantWorkflowResponse(
            code=200,
            msg="success",
            data=workflow_data
        )

        return response.model_dump()

    def _create_message(self, msg_type: str, message: str = "", progress: int = 0, **kwargs) -> str:
        """åˆ›å»ºæ ‡å‡†åŒ–çš„æµå¼æ¶ˆæ¯"""
        data = {
            "type": msg_type,
            "timestamp": datetime.now().isoformat()
        }

        if message:
            data["message"] = message
        if progress > 0:
            data["progress"] = progress

        # æ·»åŠ å…¶ä»–å‚æ•°
        data.update(kwargs)

        return json.dumps(data, ensure_ascii=False) + "\n"


# ä¸»è¦çš„å¼‚æ­¥æµå¼å·¥ä½œæµå‡½æ•°
async def async_workflow_stream_with_retrieval(state: PrengantState) -> AsyncGenerator[str, None]:
    """
    å¼‚æ­¥åŒ–å·¥ä½œæµæµå¼æ‰§è¡Œ - ä¿æŒä¸“å®¶çŸ¥è¯†è´¨é‡ï¼Œè§£å†³å¹¶å‘é˜»å¡

    ç‰¹ç‚¹ï¼š
    1. æ‰€æœ‰æ£€ç´¢æ“ä½œå¼‚æ­¥åŒ–ï¼Œæ”¯æŒçœŸæ­£é«˜å¹¶å‘
    2. ä¿æŒå®Œæ•´çš„ä¸“å®¶çŸ¥è¯†åº“æ£€ç´¢æµç¨‹
    3. åŸºäºä¸“å®¶çŸ¥è¯†è¿›è¡ŒAIç”Ÿæˆï¼Œç¡®ä¿å›ç­”è´¨é‡
    4. æµå¼è¿”å›ï¼Œç”¨æˆ·ä½“éªŒè‰¯å¥½
    """
    workflow = AsyncWorkflowWithRetrieval()
    async for chunk in workflow.execute_async_workflow_stream(state):
        yield chunk


if __name__ == "__main__":
    # æµ‹è¯•å¼‚æ­¥å·¥ä½œæµ
    import asyncio

    async def test_async_workflow():
        test_state: PrengantState = {
            "input": "å­•æœŸå¤´æ™•æ€ä¹ˆåŠï¼Ÿ",
            "maternal_id": 1,
            "chat_id": "test_async_retrieval_123",
            "user_type": "pregnant_mother",
            "timestamp": datetime.now().isoformat(),
            "file_id": []
        }

        print("=== æµ‹è¯•å¼‚æ­¥åŒ–æ£€ç´¢å·¥ä½œæµ ===")
        start_time = time.time()

        async for chunk in async_workflow_stream_with_retrieval(test_state):
            print(f"[{time.time() - start_time:.2f}s] {chunk.strip()}")

    # asyncio.run(test_async_workflow())